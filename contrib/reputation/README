INTRODUCTION

This area contains a snapshot of the ongoing development of a rudimentary
reputation system based on OpenDKIM's statistics package.

The statistics package is able to capture the flow of mail arriving as it
passes through OpenDKIM.  All messages, even unsigned ones, are recorded.
For signed messages, all signatures are recorded including pass/fail,
signing domain, signature properties, etc.  The statistics system can also
upload the locally-collected data to an arbitrary central repository where
it can be aggregated and then made available to participants to query.

This central system has not yet been designed to be queryable as a public
service, but may be in the future.  This directory contains a description
sufficient to build such a system, but also (and primarily) for building a
local store that can be used as an intelligent filter based on DKIM
verification results.

REQUIREMENTS

The requirements for this reputation system are as follows:

1) Reputation must be based on the domain(s) that have some kind of
   responsibility for the message.

2) Reputation should be expressed in such a way that it can be easily
   converted to a message flow limit.

3) There must be some reasonable, even if tiny, allowance made for
   domains about which no data have been accumulated.

4) The system may output a few values for a given domain, such as
   "strict", "medium" and "light", and these would be applied at the
   discretion of the implementation.

5) Data expiration must be configurable at the discretion of the site using
   the system.  That is, the system should neither impose a specific lifetime
   of the data accumulated, though it should discuss the impacts of different
   choices of data lifetime.

OVERVIEW

The system computes a recommended flow based on the history of the
behaviour of the signing domain(s) on a message.  Mail that is unsigned will
be treated as though it was signed by the NULL domain, meaning all unsigned
mail is presumed to come from a common source ("the unknown", as it were).
What remains, then, is to determine the perceived value of mail from each
of these sources.  This is done via data collection and statistical methods.

What is critical, then, is a comparison of two things for any given source:

(a) How much mail that source sends in a given time period; and

(b) How much of that mail is undesired (spam, phish, virus, etc.).

The first part is the easy part.  In our case, this is done via the
opendkim filter (which extracts DKIM results) and an SQL data store
(which records these results).  This sort of data allows one to construct
a view of the past behaviour of a domain in terms of sending volume only.
Various mechanisms exist such as predictive inference and time series
analysis to determine what might be expected for the next time period.
For our first implementation, we use predictive inference as it is simpler
to calculate.

The second part is the hard part.  Identifying spam in an accurate and timely
manner is the key to building effective filtering systems.  Commercial
solutions such as Cloudmark's SpamNet are highly effective specifically
because they base their accuracy not only on clever algorithms that extract
interesting properties, but also on the feedback of their users to tell
them which of those properties are typically associated with a spam message.
This also gives rise to trust in the reporters, where more value is given
to feedback from a reporter that is usually quick to report and concurs
with the majority.  However, a universal feedback system in the open source
world does not yet exist.  Thus, for this experiment, we provide some local
tools for administrators to make available to users, but primarily focus
on the use of SpamAssassin as a fast and reasonably accurate feedback system.

This means, for each arriving message, we have a set of one or more valid
DKIM signatures, and an indication of whether or not the message is
considered spam.

For the purposes of illustration of this method, we will use the counts from
a single calendar day (i.e., midnight to midnight) as a data point.

The next section describes the reputation system in detail, and the following
one explains how to configure and activate it.

DESIGN DETAILS

For each message collected by the statistics system, numerous details are
recorded.  For the full schema, see stats/mkdb.mysql and stats/README.

Let each message M have associated with it the set D of domains for which
valid signatures were present upon receipt.  D may be empty, in which case
we pretend there was one valid signature for the domain NULL.  Each M also
has a value S associated with it, which is either 0 if the message was not
labeled as spam or 1 if it is.

Thus, within a given day, any domain has a count Cm of messages that had
a valid signature on it from that domain, and a count Cs of messages that
were spam.  Obviously, for any given day, Cm >= Cs.  And over N days,
any given domain will have N of each of these.  Considering the data for
any given domain, this permits some useful analysis.

Trivially, one can compute the average daily number of messages bearing a
valid signature from any given domain, and the average number of those that
are spam.  It would then be easy to detect when the amount of spam, or the
amount of mail in general, went above average at all, or perhaps by a certain
percentage.  The simple mean, however, is too volatile for these purposes.
Something more robust (i.e., less influenced by extreme data points) is
required.

Were we to plot such data on a graph using the message count on the X axis
and the spam count on the Y axis, we could approximate the data set with a
line using some method of regression.  If the slope of that line approaches
1, then the domain tends to send spam; if it approaches zero, it tends to be
a clean source of mail.  Using this method might also be helpful to predict
a value of Cs for a new day given a value for Cm for that day.  However,
linear regression methods presuppose minimal error in measurement, and although
we are sure of our received message count, the spam count is subjective
because SpamAssassin is not as precise as we would like for this work.

This experiment instead uses predictive inference as its method of computing
results.  A prediction interval is based on the mean and standard deviation
of a data set and is used to estimate, given a set of previous values, a
range in which the next value will fall.  A typical prediction interval
for a data set, given those two values is defined by:

	[ u - o z, u + o z ]

...where:

	u ("mu") is the sample mean from the data observed to date
	o ("sigma") is the standard deviation from the data observed to date
	z ("zeta") is the standard score for the size of the prediction
	  interval desired

The experiment uses the assumption that the distribution of the data
points for any given domain is normal; that is, that the data points for
any given domain tend to cluster around a single mean value.

The prediction interval is centered around the mean, and its width is
governed by the width of the standard score and the standard deviation.

The standard score governs the width of the prediction interval.  A
prediction interval's description is what provides the "z" value above, but
not in an intuitive way: A 90% prediction interval is 90% likely to contain
the next data point, but is possibly considerably wider than a 75%
prediction interval based on the same data (so that it is 90% likely to be
right).  Thus, a less precise (tighter) interval may lead to some false
alarms, but a more precise (wider) interval may lead to no intervention at
all.  In this implementation, we leave the choice of the interval
width to the administrator of the system being protected.

The standard deviation is a measure of how erratic the behaviour of the
measured quantity is.

We introduce one additional quantity here: R, which is Cs/Cm for a given
day for a given domain; it is the ratio of spam to messages, or the percent
of mail that is spam for a given day from a given domain.

In general, we construct a prediction interval around the Cm and R data
for any given domain based on observed data to date.  This gives a low
and high end of a range within which the next daily message count and daily
spam ratio are expected to fall.

Note that we are not always interested in the lower boundary of the prediction
interval as there's no action to be taken should a daily message count or
daily spam count fall below normal (less mail and/or less spam aren't
things that require intervention), but this point will be revisited later.

Thus, at the start of day X, we compute a prediction interval for all domains
for which we have accumulated data (including the NULL domain) for the
message count and the spam ratio.  If during day X we observe that the domain
has sent more than the predicted high or the ratio of received mail to spam
exceeds the high prediction, intervention can be enacted.

This seems to be a good fit on simple analysis: If a domain sends the same
amount of mail every day, then the mean will be that value, the standard
deviation will be zero, and thus the prediction will be that same value.
As the value varies, the standard deviation increases, and this variability
grants some tolerance in the prediction interval for increases in traffic
that might not be present using a simpler model.  For the spam ratio, the
mean gives an indication of how clean the domain's stream is, and the
standard deviation a measure of how erratic the domain's behaviour typically
is.  This can be used as a measure of the trustworthiness of the stream
in terms of sending desirable content.  In fact, one could conceive of
a system that uses the low, middle and high points in the ratio's prediction
interval to impose light, moderate and aggressive rate limiting modes against
senders.

Note, though, that these statistical quantities are only really useful
once a substantial number of data points exist.  It's necessary then to
create a second system for handling domains about which there is little
information available.  This is a particularly sensitive point given the
fact that many domains are created solely to send a spam campaign and are
then abandoned, and many of these sign their mail.

Thus, the system as a whole divides domains into two categories: high-data
and low-data.  An obvious question, then, is what a good threshold is at
which a domain transitions from one to the other.  So we have two problems
to solve.

First, we need a method to determine the threshold.
