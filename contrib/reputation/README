INTRODUCTION

This area contains a snapshot of the ongoing development of a rudimentary
reputation system based on OpenDKIM's statistics package.

The statistics package is able to capture the flow of mail arriving as it
passes through OpenDKIM.  All messages, even unsigned ones, are recorded.
For signed messages, all signatures are recorded including pass/fail,
signing domain, signature properties, etc.  The statistics system can also
upload the locally-collected data to an arbitrary central repository where
it can be aggregated and then made available to participants to query.

This central system has not yet been designed to be queryable as a public
service, but may be in the future.  This directory contains a description
sufficient to build such a system, but also (and primarily) for building a
local store that can be used as an intelligent filter based on DKIM
verification results.

REQUIREMENTS

The requirements for this reputation system are as follows:

1) Reputation must be based on the domain(s) that have some kind of
   responsibility for the message.

2) Reputation should be expressed in such a way that it can be easily
   converted to a message flow limit.

3) There must be some reasonable, even if tiny, allowance made for
   domains about which no data have been accumulated.

4) The system may output a few values for a given domain, such as
   "strict", "medium" and "light", and these would be applied at the
   discretion of the implementation.

5) Data expiration must be configurable at the discretion of the site using
   the system.  That is, the system should neither impose a specific lifetime
   of the data accumulated, though it should discuss the impacts of different
   choices of data lifetime.


OVERVIEW

The system computes a recommended flow based on the history of the
behaviour of the signing domain(s) on a message.  Mail that is unsigned will
be treated as though it was signed by the NULL domain, meaning all unsigned
mail is presumed to come from a common source ("the unknown", as it were).
What remains, then, is to determine the perceived value of mail from each
of these sources.  This is done via data collection and statistical methods.

What is critical, then, is a comparison of two things for any given source:

(a) How much mail that source sends in a given time period; and

(b) How much of that mail is undesired (spam, phish, virus, etc.).

The first part is the easy part.  In our case, this is done via the
opendkim filter (which extracts DKIM results) and an SQL data store
(which records these results).  This sort of data allows one to construct
a view of the past behaviour of a domain in terms of sending volume only.
Various mechanisms exist such as predictive inference and time series
analysis to determine what might be expected for the next time period.
For our first implementation, we use predictive inference as it is simpler
to calculate.

The second part is the hard part.  Identifying spam in an accurate and timely
manner is the key to building effective filtering systems.  Commercial
solutions such as Cloudmark's SpamNet are highly effective specifically
because they base their accuracy not only on clever algorithms that extract
interesting properties, but also on the feedback of their users to tell
them which of those properties are typically associated with a spam message.
This also gives rise to trust in the reporters, where more value is given
to feedback from a reporter that is usually quick to report and concurs
with the majority.  However, a universal feedback system in the open source
world does not yet exist.  Thus, for this experiment, we provide some local
tools for administrators to make available to users, but primarily focus
on the use of SpamAssassin as a fast and reasonably accurate feedback system.

This means, for each arriving message, we have a set of one or more valid
DKIM signatures, and an indication of whether or not the message is
considered spam.

For the purposes of illustration of this method, we will use the counts from
a single calendar day (i.e., midnight to midnight) as a data point.  One
could use any time quantum, but days and hours seem to be the most likely
to reveal patterns or be equitable to use of time series analysis, which will
be tried in a later paper.

The next section describes the reputation system in detail, and the following
one explains how to configure and activate it.


DESIGN DETAILS

For each message collected by the statistics system, numerous details are
recorded.  For the full schema, see stats/mkdb.mysql and stats/README.

Let each message M have associated with it the set D of domains for which
valid signatures were present upon receipt.  D may be empty, in which case
we pretend there was one valid signature for the domain NULL.  Each M also
has a value S associated with it, which is either 0 if the message was not
labeled as spam or 1 if it is.

Thus, within a given day, any domain has a count Cm of messages that had
a valid signature on it from that domain, and a count Cs of messages that
were spam.  Obviously, for any given day, Cm >= Cs.  And over N days,
any given domain will have N of each of these.  Considering the data for
any given domain, this permits some useful analysis.

Trivially, one can compute the average daily number of messages bearing a
valid signature from any given domain, and the average number of those that
are spam.  It would then be easy to detect when the amount of spam, or the
amount of mail in general, went above average at all, or perhaps by a certain
percentage.  The simple mean, however, is too volatile for these purposes.
Something more robust (i.e., less influenced by extreme data points) is
required.

Were we to plot such data on a graph using the message count on the X axis
and the spam count on the Y axis, we could approximate the data set with a
line using some method of regression.  If the slope of that line approaches
1, then the domain tends to send spam; if it approaches zero, it tends to be
a clean source of mail.  Using this method might also be helpful to predict
a value of Cs for a new day given a value for Cm for that day.  However,
linear regression methods presuppose minimal error in measurement, and although
we are sure of our received message count, the spam count is subjective
because SpamAssassin is not as precise as we would like for this work.

This experiment instead uses predictive inference as its method of computing
results.  A prediction interval is based on the mean and standard deviation
of a data set and is used to estimate, given a set of previous values, a
range in which the next value will fall.  A typical prediction interval
for a data set, given those two values is defined by:

	[ u - o z, u + o z ]

...where:

	u ("mu") is the sample mean from the data observed to date
	o ("sigma") is the standard deviation from the data observed to date
	z ("zeta") is the standard score for the size of the prediction
	  interval desired

The experiment uses the assumption that the distribution of the data
points for any given domain is normal; that is, that the data points for
any given domain tend to cluster around a single mean value.

The prediction interval is centered around the mean, and its width is
governed by the width of the standard score and the standard deviation.

The standard score governs the width of the prediction interval.  A
prediction interval's description is what provides the "z" value above, but
not in an intuitive way: A 90% prediction interval is 90% likely to contain
the next data point, but is possibly considerably wider than a 75%
prediction interval based on the same data (so that it is 90% likely to be
right).  Thus, a less precise (tighter) interval may lead to some false
alarms, but a more precise (wider) interval may lead to no intervention at
all.  In this implementation, we leave the choice of the interval
width to the administrator of the system being protected.

The standard deviation is a measure of how erratic the behaviour of the
measured quantity is.

We introduce one additional quantity here: R, which is Cs/Cm for a given
day for a given domain; it is the ratio of spam to messages, or the percent
of mail that is spam for a given day from a given domain.

In general, we construct a prediction interval around the Cm and R data
for any given domain based on observed data to date.  This gives a low
and high end of a range within which the next daily message count and daily
spam ratio are expected to fall.

Note that we are not always interested in the lower boundary of the prediction
interval as there's no action to be taken should a daily message count or
daily spam count fall below normal (less mail and/or less spam aren't
things that require intervention), but this point will be revisited later.

Thus, at the start of day X, we compute a prediction interval for all domains
for which we have accumulated data (including the NULL domain) for the
message count and the spam ratio.  If during day X we observe that the domain
has sent more than the predicted high or the ratio of received mail to spam
exceeds the high prediction, intervention can be enacted.

This seems to be a good fit on simple analysis: If a domain sends the same
amount of mail every day, then the mean will be that value, the standard
deviation will be zero, and thus the prediction will be that same value.
As the value varies, the standard deviation increases, and this variability
grants some tolerance in the prediction interval for increases in traffic
that might not be present using a simpler model.  For the spam ratio, the
mean gives an indication of how clean the domain's stream is, and the
standard deviation a measure of how erratic the domain's behaviour typically
is.  This can be used as a measure of the trustworthiness of the stream
in terms of sending desirable content.  In fact, one could conceive of
a system that uses the low, middle (mean) and high points in the ratio's
prediction interval to impose light, moderate and aggressive rate limiting
modes against senders.  We can call these Plow, Pmed, Phigh; the one a site
chooses to use is simply called P.

A simpler model combining these is possible: On day X, allow through a
quantity of mail from a domain equal to the high end of its prediction
interval on Cm multiplied by the quantity (1 - P).  Where the standard
deviation around R is larger than the mean, this quantity can be negative;
in that case, we simply use a value of zero, effectively cutting that domain
off; it has earned itself a ban.

Note, though, that these statistical quantities are only really useful
once a substantial number of data points exist.  It's necessary then to
create a second system for handling domains about which there is little
information available.  This is a particularly sensitive point given the
fact that many domains are created solely to send a spam campaign and are
then abandoned, and many of these sign their mail.

Thus, the system as a whole divides domains into two categories: high-data
and low-data.  An obvious question, then, is what a good threshold is at
which a domain transitions from one to the other.  So we have two problems
to solve.

First, we need a method to determine that threshold.  In a simple system,
a fixed time limit could be imposed, i.e. a domain transitions from
low-data to high-data after having some predetermined number of days of
activity recorded.  We would like to do something a little more automated.
For now we will use the fixed time limit and return to this question later.

If the current day is X(0), let N be the fixed time threshold in days.
Thus the low-time threshold is at day X(N).  Compute the mean and standard
deviation for the Cm and R values across all signing domains for data from
X(1) to X(N).  This gives us the ability to construct a prediction interval
for those values in the aggregate across all low-time domains.  This quantity
is thus a measure of how we expect new domains to behave given the observed
behaviour of other new domains.  Therefore, we enforce the prediction
interval of Cm on low-time domains, modulated by R as above.  As a tiny
measure of "benefit of the doubt", a flow limit computed in this way to be
zero is modulated further to permit some small (perhaps fixed, perhaps based
on a ratio) quantity of mail until a single spam message is detected, at
which point no further mail is allowed that day.

Now we revisit the question of what N should be.  The statistics package
can be used to determine the first and last time a domain name was seen
used in a signature.  So for each domain name we also compute L, the
difference in days between "first seen" and "last seen".  This gives us yet
another data set upon which to compute a mean and standard deviation.
If we constrain this to include only L values for domains that send mostly
spam, we can see how long a spam-generating domain tends to live before
being abandoned for attracting attention.  For the deployed test system,
we set N using a simple value of twice the average of L; the standard
deviation in this case is ignored.  An alternate approach is to compute a
prediction interval around the mean L value; the high end value becomes
the value used for N.

Next we need to design the system so that messages that are disallowed
because of the above rate controls are still counted in that domain's
aggregated data.  A "temporary failure" result returned in that case may
cause a re-send attempt; this is not to be counted as a separate message
in order to get a real view of what that domain attempted to send.

We finally need to define an initial state, where the system doesn't know
anything.  For simplicity, we will run the system in wide-open data
collection only mode for 30 days prior to doing any rate throttling.


SUMMARY

Given a set of numbers, one can compute the well-known statistical
quantities know as the mean (the average value of the set) and standard
deviation (a measure of the dispersal of values about the mean) as useful
tools to describe the set.

Select a prediction interval width to be used for the system (e.g.,
75%, 90%, or 95%).  Using a lookup table, this gives the standard score, "z",
to be used at various points in this algorithm.

Select a ratio that indicates a relatively unknown domain that sends more
than that ratio of spam is considered undesirable.  Call this S.

Select Mmin, a minimum number of messages to allow in per day from an unknown
source regardless of calculated limits.

Select a strictness setting, "light", "medium" or "strict".

Let D be a set of domain names.  It is further partitioned into Dlow and
Dhigh, these being, respectively, domains about which we have only a small
amount of data and those about which we have accumulated a useful amount
of data.

For each M received, record the following details:

	- the set of domain names taken from valid DKIM signatures, if any
	- the time at which the message was received
	- a Boolean indication of whether or not the message was spam

Partition these records into "buckets", one per day.  We can now compute
the following quantities about any given domain or about the system as a
whole, and across all observed days or for any given day

	- total messages sent (Cm)
	- total spam sent (Cs)
	- count of days for which data are available

For each domain D, compute the following quantities:

	- mean messages sent (uCm)
	- mean spam messages sent (uCs)
	- mean spam ratio (uR = uCs / uCm)
	- the time, in days, between this domain's first use in a valid
	  signature and its last use in a valid signature (L)

Discard those for which uR is less than S or uCm is less than 2.

Compute the mean and standard deviation for L, and then a prediction interval
around uL.  Denote the high end of this prediction interval to be N.
Partition the set of known domains D into Dlow and Dhigh based on whether
or not L for a given domain is above or below this value.

Compute a prediction interval around uCm and uR for all domains in Dlow.
Denote the high end of the prediction interval around uCm as Chigh, and
the high end of the prediction interval around uR as Rhigh.  Impose a
per-domain overall message rate on domains in Dlow defined by:

	M = Chigh * (1 - Rhigh)

Furthermore ensure that a given domain does not exceed a daily spam ratio R
selected based on the strictness setting: use Rlow for "light", uR for
"medium", or Rhigh for "strict".

This allows messages to be sent by a low time domain up to what it might
legitimately send regardless of content, modulated downward by the average
low time domain's expected behaviour.  If most low time domains send
undesirable mail, this number will approach zero or even be negative.
If that's the case, the hard-coded minimum rate Mmin is used, except that
no further mail is accepted once a single piece of spam is observed.

Next, compute a prediction interval around uCm and uR for each domain in
Dhigh.  Denote the high end of the prediction interval around a domain's Cm
as Chigh, and the high end of the prediction interval around R as Rhigh.
Impose a per-domain overall message rate on domains in Dhigh defined by:

	M = Chigh * (1 - Rhigh)

Similarly restrict the spam ratio for each high-time domani according
to Rlow, uR or Rhigh as described above.

This allows messages to be sent by a high time domain up to what it might
legitimately send regardless of content, modulated downward by that domain's
expected behaviour.  Unlike for the low time domains, this is not dependent
on the behaviour of other domains.  A domain that earns a high enough value
of Rhigh can cause its daily limit to be modulated down to zero.


DISCUSSION

The standard deviation in the computation of Rhigh acts as a measure of
trust in the domain's behaviour; the more erratic its spam behaviour is,
the higher the standard deviation becomes, the larger the value of Rhigh
gets, and the more Chigh is penalized.

The more well-behaved a domain is, the more leeway it is given.  It is
presumed that any domain will have some variability in its Cm values, meaning
it will have a standard deviation around C, meaning Chigh will be bigger
than average.  A well-behaved (low Rhigh) domain, then, will implicitly give
the domain room to "grow" as it continues to behave well.

A domain that is well-behaved for a long period will have relatively low
values for Rlow, uR and Rhigh.  Thus, since both an overall message rate
and a spam rate are enforced, a sudden surge in spam from a typically
well-behaved domain will be intercepted by the enforcement of the R-limits
even if there's lots of room left in a given day to play with the M limit.

This system obviously creates a long-term incentive for a particular signing
domain to behave well and do so consistently.

When a threshold is reached or exceeded, it is important to log the
delivery attempt details so that a view of what the signing domain
ultimately presented for delivery is recorded; throttling mail without doing
so leads to falsely low data for the current day.  It is difficult to do this
precisely where SMTP-level rejection (especially 4xx, or temporary failure
codes, which invite the sender to re-try) is done, but a reasonably good
estimation can be made by recording the actual DKIM signature (i.e., the
value of the "b=" tag), which is highly unlikely to suffer collisions,
coupled with the intended recipient(s) of the message.  The value of the
Message-ID field is also likely useful for this purpose since an attacker
would be unlikely to recycle these lest subsequent deliveries by caught by
filters that attempt to detect and suppress duplicates.

The study did not consider the impact of a time range on the data
used as input.  The cost of storing infinite data is non-trivial, so it
is likely a production system would have to discard old data after some
period.  This means the various computations of means and standard deviations
will only include data for a fixed period, meaning periods of good or bad
behaviour beyond such a time window will not be factored into the results.



THREAT ANALYSIS

The most obvious question one must ask when presenting a system like this
is: How can one try to game this system?  Put another way, if someone
wanting to maximize delivery through this system had the above details
and/or the source code, what could that person do to get the most mail
through?  The answer comes from analyzing how the two filtering limits 
are computed.  First, we consider them separately.

The simplest is the spam ratio being enforced.  Based on operator preference,
this will be either Rlow, uR or Rhigh.  In any case, the number needs to be
as high as possible.  For the uR case, that means having a history of sending
as much spam as possible.  For Rlow, which is uR minus the product of the
standard deviation and the standard score, the goal is to maximize uR and
minimize the standard deviation, so constantly send a high ratio of spam.
For Rhigh, which is uR plus the product of the standard deviation and the
standard score, the goal is to maximize both uR and the standard deviation,
which means send a high ratio overall while also having somewhat erratic
behaviour, meaning some days the ratio is relatively low and sometimes it
is relatively high.  In general, maximizing this value means the system will
expect the domain to send a lot of spam, so then the next day the same
expectation is established and then a lot of spam doesn't get noticed as
easily.  Conversely, a well-behaved domain will have relatively low
values of Rlow (low uR and small standard deviation), uR, or Rhigh (low uR
and larger standard deviation, i.e., a low average ratio but erratic
behaviour), and then if such a domain starts to send spam suddenly, that
day's R value will exceed any of those thresholds rather quickly.  Thus,
to beat this part of the system, one needs to have a history of sending
a high ratio and somewhat erratic behaviour.

The other limit enforced is the overall flow limit.  Recall that this
is computed using:

	M = Chigh * (1 - Rhigh)

Recall also that Chigh is the high-end of a prediction interval around the
total mail volume.  Chigh is equal to uC (average mail volume overall)
for that domain plus the product of the standard score and the standard
deviation of the domain's flow data.  Thus, Chigh is maximized by having a
high daily average message throughput coupled with erratic daily volumes.

The obvious goal then is to maximize both Chigh and (1 - Rhigh), meaning the
domains that can get the most mail through are those that tend to send a lot 
but don't have very consistent daily volumes, and have as small a value
of Rhigh as possible.

Since the flow limit and the spam ratio limit are enforced together, they
must be considered together.  Note, then, that of the two filtering methods
used here, they counteract each other when spam is present: maximizing
Rhigh minimizes (1 - Rhigh) and thus minimizes M.  Moreover, to earn a high
M limit, a spammer has to send a lot of mail over time (to get Chigh up) that
does not attract negative attention (which would get Rhigh up).  In the
face of an effective user community with a functioning user feedback system,
this would be a difficult and expensive task indeed.  Since the efficacy of
spam is a game of economy wherein a small expense in sending a huge volume
provides a relatively high return, making a succesful spam campaign more
expensive prospect is a likely win.

There is probably a "sweet spot" that minimizes the expense of getting
a relatively high M limit and low Rhigh limit.  For a given target M, we want
Chigh to be maximal and Rhigh to be optimal.  For every fraction that
Rhigh increases, (1 - Rhigh) proportionally decreases, so Chigh must
proportionally increase.  So if, for example, Rhigh goes from 0.1 to 0.2,
(1 - Rhigh) goes from 0.9 to 0.8; a corresponding increase of 0.8/0.9 or
89% in Chigh would have to occur to preserve the same value of M.  Generally,
to counteract any increase in Rhigh requires a substantial increase in
Chigh to preserve the same value of M.

To illustrate this, the following table shows the percent increase in
Chigh one would need to offset a change in Rhigh from the line shown to
the one before it:

Rhigh	(1-Rhigh)	%d Chigh
-----	---------	--------
0.00	1.00
0.05	0.95		95.0000
0.10	0.90		94.7368
0.15	0.85		94.4444
0.20	0.80		94.1176
0.25	0.75		93.7500
0.30	0.70		93.3333
0.35	0.65		92.8571
0.40	0.60		92.3077
0.45	0.55		91.6667
0.50	0.50		90.9091
0.55	0.45		90.0000
0.60	0.40		88.8889
0.65	0.35		87.5000
0.70	0.30		85.7143
0.75	0.25		83.3333
0.80	0.20		80.0000
0.85	0.15		75.0000
0.90	0.10		66.6666
0.95	0.05		49.9999

This shows an increment rate of 0.05 in Rhigh.  Using larger steps does not
improve this much for the attacker; for example:

Rhigh	(1-Rhigh)	%d Chigh
-----	---------	--------
0.00	1.00
0.20	0.80		80.0000
0.40	0.60		75.0000
0.60	0.40		66.6667
0.80	0.20		50.0000
1.00	0.00		0.0000

The increase in Chigh to offset any increase in Rhigh is substantial.  So
in general, running a relatively clean ship and then trying to get a higher
amount of spam through suddenly without hitting limits requires substantial
ramp-up of clean mail with some spam mixed in.  This certainly seems to make
success for a small amount of spam a relatively expensive prospect, which
probably removes the incentive.

This analysis thus far relates to high-time domains.  Since the math is
the same, the same sort of conclusions apply to low-time domains, except
that finding an optimal value for Rhigh would require co-ordinating that
effort across all other low-time domains also probably creates a sufficiently
high bar that it would not be worth the effort.


EXPERIMENTAL DATA

OpenDKIM collected over 16.5M message records and just under 5M signature
records in a nine-month period to provide input for this study.  21 reporting
sites provided data, spread over three continents.

First, the low-time range calculation:

+---------+--------------+--------------+---------------+-----------------+
| domains | min duration | max duration | mean duration | duration stddev |
+---------+--------------+--------------+---------------+-----------------+
|    1258 |            0 |          239 |        7.6932 |         17.7871 |
+---------+--------------+--------------+---------------+-----------------+

Using 75% prediction intervals, the standard score is 1.16, so the low-high
threshold is computed by:

	T = 7.6932 + 17.7871 * 1.15
	  = 7.6932 + 20.4552
	  = 28.1484

Behaviour of domains younger than this results in the following values:

     domains: 1543
          uC: 51.2541
   stddev(C): 85.8353
       Chigh: 149.9646
          uR: 0.29100661
   stddev(R): 0.28465461
       Rhigh: 0.61835941
           M: 88.92600565

Thus individual young domains in would be permitted to send no more than
89 messages each, and no more than 61.8% of it can be spam.  Exceeding either
limit would disallow any more mail from those domains in the current day.

Some example high-time domains:

+------------------+---------+----------+-------+------+-------+------+-------+
| name             | uC      | sd(C)    | Chigh | uR   | sd(R) | Rhi  | M     |
+------------------+---------+----------+-------+------+-------+------+-------+
| NULL             | 14700.7 | 20234.66 | 37971 | 0.26 |  0.18 | 0.47 | 20205 |
| gmail.com        |   583.4 |   762.00 |  1460 | 0.01 |  0.04 | 0.06 |  1378 |
| yahoo.com        |   494.0 |   586.01 |  1168 | 0.20 |  0.33 | 0.58 |   493 |
| blackops.org     |     1.7 |     1.20 |     3 | 0.00 |  0.00 | 0.00 |     3 |
| opendkim.org     |     6.9 |     7.45 |    15 | 0.02 |  0.08 | 0.10 |    14 |
| yahoogroups.com  |   184.7 |   162.91 |   372 | 0.03 |  0.15 | 0.21 |   295 |
| googlegroups.com |    49.5 |    57.62 |   116 | 0.07 |  0.24 | 0.34 |    76 |
| paypal.com       |    70.2 |    62.52 |   142 | 0.00 |  0.00 | 0.00 |   142 |
| linkedin.com     |   232.8 |   243.93 |   513 | 0.00 |  0.00 | 0.00 |   511 |
| returnpath.net   |     1.4 |     0.49 |     2 | 0.00 |  0.00 | 0.00 |     2 |
| orangefinder.com |    16.4 |     4.45 |    22 | 1.00 |  0.00 | 1.00 |     0 |
| gooldlangi.com   |    62.4 |    48.89 |   119 | 0.62 |  0.30 | 0.97 |     4 |
+------------------+---------+----------+-------+------+-------+------+-------+
(Some column names have been shortened to fit the page.)

The "NULL" domain represents unsigned mail.

Low values of uR represent relatively clean streams, while two domains at
the bottom send mostly or almost entirely spam and are thus severely rate
limited.  Unsigned mail receives no benefit from its erratic volume nature
(the high standard deviation on C) because its spam "Rhi" value counteracts
that property.

We can see that domains which tend to behave well (those with low "Rhi"
values) are given high limits relative to their average volumes, while those
that do not (with high "Rhi" values) have their overall flow limits greatly
reduced corresponding to the resulting mistrust.  Moreover, a surge in
spam volume from an otherwise well-behaved source would be restricted because
"Rhi" would be quickly exceeded.


EXPERIMENTAL SETUP

[attach schemae and queries used above here]


SETUP INSTRUCTIONS

[provide details about how to set up a local instance of the reputation system
 described above here]


TROUBLESHOOTING

[provide some troubleshooting tips]


SUPPORT/QUESTIONS/COMMENTS

[provide contact info]
